\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{apacite}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
%\usepackage{paralist}
%\usepackage{mdwlist}
\newcommand{\comment}[1]{}
\bibliographystyle{apacite}

\begin{document}
\title{Final Project Proposal}
\author{Andrew Stromme \& Ryan Carlson}
\date{April 6, 2010}
\maketitle

\section{Introduction}

At a high level, our objective with this project is to implement a system that will allow an AIBO to identify and accurately track objects in its visual environment. We have seen that simple `blobify' filters allow the user to select a certain color and have the robot focus on the largest instance of that color. But in a real-life environment, objects are constantly moving around the plane of view. Moreover, they are moving closer and further away from the robot and may even change color if moved into different lighting conditions. Here blobify would clearly fail us by selecting the new largest item of the specified color. We intend to implement a Growing Neural Gas (GNG) that is able to categorize objects by grouping regions of similar color and close proximity. We expect the GNG will enable an AIBO to track objects in a realistic environment.

We plan to use the GNG algorithm to identify and categorize blobs of similarly colored/positioned pixels within an image. GNG is good at making categorizations and simplifications based on distance in the N-dimension feature space it is given. After the GNG has categorized the blobs present in the static image we will change the frame to a slightly different image where some of the objects have moved. This relates well to the real world example where robots see incremental frames in which objects move slowly around the robot's field of view. Because GNGs are also good at adapting to changes in the input signal distribution we hope that the categorized blobs will be tracked by the GNG's nodes and edges.

\section{Implementation}
To implement this detection and tracking a small layer on top of GNG is needed. For a given frame this layer will extract vectors corresponding to position points on the image. For each point, the vector will tentatively consist of 5 dimensions, the x position, y position, red, green, and blue (RGB) values. The wrapper layer will pass into the GNG randomly selected input signals from the pool of these extracted vectors. This will allow the GNG to create and move nodes and edges to classify the similarities (in color or in physical distance) between input vectors. GNGs tend to create interconnected nodes that map similarities, but have no edge links between nodes that are far apart in the input space. Because of these edge links the resulting nodes and edges can be split up into `blobs' that the GNG has detected. The wrapper will traverse the graphs that the GNG has created and give the higher level control the blobs that have been detected.

After a (large) number of time steps the blobs that have been detected by the GNG should settle down. Once this happens it is time to move to the next frame. We assume that each new frame will be very similar to the one that came before it because our intended environment is the real world with a fast capture rate. For the new frame, the GNG will again be run on selected input signals as described above. However, the GNG topology will be a continuation of the nodes and edges created from the previous frame. This should in effect provide a sort of bootstrapping for the GNG because all it has to do is shift the existing nodes over or shrink them slightly. Additionally, this will allow for the tracking of blobs across frames because most of the nodes and edges will stay the same. 

We think that there might be an issue with leaving `old' structures behind if the content in the frames changes or moves too quickly. To compensate for this we plan to assign an age to each node as a record of what frame the network was in when that particular node or edge was last updated. This will allow the pruning of `ghost' structures based on their age.

This implementation can translate to some form of object tracking and following on the AIBO robots. If the modified GNG described above uses the camera input from the AIBO's cameras then the robot can be given a higher level understanding of what objects are in its vision. From here, we can select a single object to track and then move the robot's head accordingly to try and keep it in the center of its vision.

\section{Stages}

We can break the process into two large sections: static object detection and dynamic object tracking. In the former, the focus will be on training the GNG to accurately categorize images. In the latter case, we will deal with moving images with objects that displace over time. We will work on static detection first. This will give us a strong sense of the capabilities of the GNG and how to use the categories to make the AIBO look at an object. After that part is working we can move onto moving images. The challenge here will be making sure the GNG accurately reflects the positions of the objects over time.

\subsection{Static Object Detection}

This stage can be broken down into three subtasks.

\begin{enumerate}
  \item Create a very simple, easy to categorize image using software like GIMP or Photoshop. The image will consist of a red box, a green box, and a blue box on a white background. The boxes will be well defined and will be pure in color. Since there will be no noise, we anticipate that the GNG will be able to classify this image without trouble.
  \item Use real-life images taken from the AIBO. We will have a range of environments from which pictures can be taken. One will be similar to the above, with the boxes drawn in marker on a white board. Another will be various objects placed around the robot lab, which will be a bit noisier and will have less contrast between foreground and background.
  \item Finally, we will use the categories generated by the GNG to make the AIBO attend in the direction of a specific object.
\end{enumerate}

\subsection{Dynamic Object Tracking}

This stage has three analogous subtasks as the object detection.

\begin{enumerate}
  \item Create a sequence of computer-generated images where colored boxes incrementally move around a white background. Again, this input is noise free and will allow us to fine-tune the GNG for moving images.
  \item Use AIBO-provided videos of real-world environments. We will show objects like a bright green water bottle moving slowly across the scene or even another AIBO walking past.
  \item Lastly, we will again use the categories given by the GNG. Here, however, the categories will be changing and we hope to track these objects using the AIBO's head.
\end{enumerate}


\end{document}
