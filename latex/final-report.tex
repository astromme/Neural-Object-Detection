\documentclass{article}
%\documentclass[twocolumn]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{apacite}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}
%\usepackage{paralist}
%\usepackage{mdwlist}
\newcommand{\comment}[1]{}
\bibliographystyle{apacite}

\usepackage{float}
\newfloat{Algorithm}{h!}{}{}

% program-related commands
\usepackage{program}
\renewcommand{\|}{\origbar} % use this instead of '|' because program package redefines '|'
\renewcommand{\WHILE}{\mbox{{\bf while} }\tab}
\renewcommand{\FOR}{\mbox{{\bf for} }\tab}
\renewcommand{\IF}{\mbox{{\bf if} }\tab}
\newcommand{\IN}{\mbox{ {\bf in} }}

\newcommand{\xyspace}{{\em xy} space}

\begin{document}
\title{Object Detection with aGiNG}
\author{Andrew Stromme \& Ryan Carlson}
\date{May 14, 2010}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Talk about using a modified Growing Neural Gas to track objects. Why is this cool? $-->$ Don't need any type of edge detection. Computationally inexpensive (amortized). We see this as a replacement for the 'blobify' filter.

\section{Related Work}

Talk about Growing Neural Gas papers by Fritzke and maybe the CBIM paper by Meeden. Also mention some previous attempts at object tracking -- Canny/Sobel edge detection. AdaBoost?

\section{Goals}

We want to create a Growing Neural Gas that can identify objects and then have the AIBO track it. Talk about limitations of blobify and how this improves it.

\section{Growing Neural Gas}

\subsection{libgng}

Libgng is an implementation of the Growing Neural Gas algorithm in C++ and Qt \footnote{The Qt libraries are used throughout libgng and gngviewer \url{http://qt.nokia.com/}.} designed specifically for dynamic inputs such as frames from a movie or camera. The library uses threading extensively so that the GNG can continue to iterate as its inputs are changed. There are a few key components of libgng, namely the GNG itself and the PointSource.

\subsubsection{Asynchronous GNG}

The GNG contained within libgng is designed to be flexible and not domain-specific. It can take input vectors of arbitrary lengths and minimum-maximum values, specified when the GNG is created. The GNG can be run asynchronously or synchronously; when running asynchronously it periodically emits information about its current state. Parameters can be changed on the fly even while the GNG is running and will take effect on the next iteration. //TODO: If we talk about dynamic error adjustment put in a reference.

\subsubsection{PointSource}

The GNG relies on randomly selected input vectors to create and move its nodes and these input vectors are generated by a class called a PointSource. The PointSource interface specifies the methods necessary for the GNG to function but does not implement them itself. Instead, these duties are taken care of by subclasses of the PointSource. Currently we have sources for static images, moving images (a series of static images), webcam images (taken in realtime from a computer's webcam) and from the AIBO's camera (over the wifi connection).

\subsection{libaibo}

Libaibo is a small C++ library that is designed to be a simple and high level interface to the AIBO. It uses the remote control functionality of the Tekkotsu Framework \footnote{TODO: Complete \url{http://www.tekkotsu.org/}} and currently allows access to the camera data and head/body controls. The library also provides a PointSource that takes points from the AIBO's camera. This is meant as a plug and play module to drop into a GNG instance. 

\section{Creating and Visualizing the Growing Neural Gas}

\subsection{Inputs}

%five-dimensional vector (x,y,hue, saturation, lightness). Introduce notions of ``object space'' as a combination of ``color space'' versus ``euclidean space''

Objects are defined as a combination of colors and distances between points in an image. Intuitively, two pixels that are close to each other and are similar colors should be judged as part of the same object. Thus we have notions of ``color space'' and ``\xyspace'' which combine to create ``object space.'' If pixels are close in both color space and \xyspace{} then there are also close in object space -- they are part of the same object. To define this object space the GNG takes as input a five-dimensional vector. This vector contains the $x$~and~$y$ coordinates as well as the color information, represented by $hue$, $saturation$, and $lightness$ (described in section \ref{sec:colorModel}). Thus when the distance in 5-space of two input vectors is measured to be small, we can say with confidence that the two vectors describe the same object. When the distance is large, the vectors are probably not part of the same object. All values are scaled from 0 to 1 before being sent to the GNG.

\subsection{Color Model (AS)}
\label{sec:colorModel}

It's important for similar colors to be represented by vectors that are close to each other. Because we are using euclidean distance this means that the 3 scalars representing each color need to be somewhat similar to those of other colors. Some of the color models that exist are Red Green Blue (RGB) where each color is created by mixing various parts of red/green/blue together. White is 255, 255, 255 and black is 0, 0, 0. The problem with this colorspace is that 
HSL. Compare to RGB and HSV. Image here?


\subsection{Distance Calculation}

%We want to place emphasis on distance. Two colors with zero difference in color space but high dist in euclidean space were being categorized as the same object. Want to make them different. Could try to find an image (4 pieces of paper?) that regular x,y dist fails but 1.5*x,y works.

%Since the GNG treats each of the five inputs equally, sometimes vectors separated by a large distance but with very similar colors end up having a small distance in object space and so the two vertices end up categorized as part of the same object. To fix this problem we added some emp

\subsection{gngviewer}

Because of the incremental nature of our modifications to the GNG we needed a solid way of visualizing the results of the GNG. From this need the gngviewer emerged. The viewer is domain-specific, it assumes a GNG with inputs that contain pixel information. The viewer runs asynchronously from the GNG and provides a continuously updating view of how the GNG is changing. It provides layers that show the nodes along with their color information, the edges and the subgraphs that have been generated. TODO: Insert image of GNGViewer.


\section{Modifications to Growing Neural Gas}

\subsection{Aging Growing Neural Gas (AGiNG) (AS)}

Since edge ages are set to zero when they win, we want a sense of how long an edge has existed. Added total age counter that is incremented at each step and is reset when removed.

\subsubsection{Removing Edges that are too old}

\subsubsection{Not counting edges that are too young}

\subsection{Color Barrier Across Object Boundaries (RC -- Draft)}

%If color difference is too great between two nodes, don't add an edge. Since we describe objects in terms of their color, makes sense.
Even with the modifications described above, experiments showed that the GNG was still adding an large number of edges between nodes that were right along the boundary edges of objects. For example, if the GNG were categorizing a scene with a black wallet against an off-white wall, a node from the foreground and from the background would often be connected since they were very close in distance. To solve this problem, a check was put in place each time an edge was to be added or the age of an existing edge was to be reset. The hues of the nodes are compared, and if their difference is greater than a specified value the edges are not created. Through experimentation we found that a good threshold value is 0.1, which corresponds to a 36 point difference in the 360-degree hue space. As a rough heuristic, this is about half way between any two consecutive bands of the color strip ({\bf reference color strip below? put this section below Methods?})

After implementing this change, we quickly saw our graph dwindle down to a size of zero. Since the early stages of the GNG consist of creating nodes between spaces that almost always differ in hue values greater than the threshold, very few or no nodes were being created. We resolved that this processing step had to be applied only once the GNG was more developed. This led to the additional check of average error against the target error. If average error is above target error, nodes are no longer being added to the graph, so it is safe to either disallow connections or the resetting of age (which can lead to an edge's removal). With this additional condition, results improved markedly. Edges between objects were much better defined and subgraphs representing each object were more likely to remain disjoint. ({\bf add image showing the difference?})

\subsection{More stuff, I think\dots}

\section{Object Tracking (RC -- Draft)}

%How we define objects -- subgraphs
Once the GNG has built up a graph representing the object space, we want to automatically identify and track objects in the scene. We can define objects in the graph in terms of subgraphs. Since we are relying on distance and color as markers of objects and are feeding that information into the Growing Neural Gas, over time the graph will partition itself into smaller, disjoint subgraphs representing objects in the scene. Thus each subgraph represents a single object that we can identify and track.

\subsection{Subgraph Generation}

In order to eventually track objects we must first identify them. Thus we need to have a method of finding the disjoint subgraphs that constitute our objects. The algorithm that accomplishes this is a layer on top of breadth-first search. It starts by picking a random vertex in the graph and looking at which vertices are reachable from it. Once no more vertices are reachable from the initial selection, we check if any vertices remain unvisited and, if so, repeat the process. The algorithm is described as pseudocode in Algorithm \ref{alg:detectingSubgraphs}.

\begin{Algorithm}[h!]
\begin{program}
  |create empty list of subgraphs, | subgraphList \rcomment{(Will contain all subgraphs)}
  \WHILE |graph contains unvisited nodes|:
    V = | an unvisited vertex in graph|
    |add | V | to queue, | q
    |create empty list, | subgraph \rcomment{(Keeps track of current disjoint subgraph)}
    \WHILE q | is not empty|:
      search = |takeFirstElement in | q \rcomment{(Removes and returns first element)}
      |mark | search | as visited|
      |add | search | to | subgraph
      |add neighbors of | search | to | q \untab
  |add | subgraph | to | subgraphList
\end{program}
\caption{Pseudocode for Detecting Subgraphs}
\label{alg:detectingSubgraphs}
\end{Algorithm}

We note that this algorithm is efficient and fast enough so that we can run it every time we update the visualizer without slowdown. We store the unvisited nodes in a hash table for constant time access and removal at some cost to space. The runtime of the algorithm is $O(\|V\|+\|E\|)$ with $\|V\|$ vertices and $\|E\|$ edges. The space requirement is $O(\|V\|+\|E\|+\|G\|)$ for the graph $G$. Since the size of graph generated by the GNG is rarely more than a hundred nodes, time and space requirements are very reasonable.

\subsection{Tracking Subgraphs}

%Pick an exemplar, track exemplar by counting number of intersecting nodes. Pseudo Code
Once we have a set of subgraphs that details the objects the GNG has identified, we need a way of tracking them. As an object moves through the scene, the associated subgraph will also translate to best fit the new object space. Some nodes will be added to fill the space while others which are no longer useful will be deleted. But in general the subgraph associated with an object at time $t_0$ will share most of the nodes as the subgraph associated with the same object at $t_1$. We can formalize this intuitive notion of subgraph tracking by choosing an exemplar subgraph at $t_0$ to track. Then, at $t_1$, we recompute the subgraphs and compare each new subgraph to the exemplar. Whichever subgraph shares the most points with the exemplar becomes the new exemplar. The algorithm is again described as pseudocode in Algorithm \ref{alg:trackingSubgraphs}.

\begin{Algorithm}[h!]
\begin{program}
  |choose an exemplar if one has not been chosen|
  maxCount = 0
  \FOR currentSubgraph \IN subgraphList: \rcomment{($subgraphList$ from subgraph generation})
    count = 0
    \FOR node \IN currentSubgraph:
      \IF node | is in | exemplar:
        count|++| \untab \untab
    \IF count > maxCount:
      maxCount = count
      exemplar = currentSubgraph
\end{program}
\caption{Pseudocode for Tracking Subgraphs}
\label{alg:trackingSubgraphs}
\end{Algorithm}

We can again look at the runtime and space requirements for the algorithm. We need to traverse every node in the graph and then check if that node is in the exemplar. Thus this has a runtime of $O(\|G\|*\|exemplar\|)$. In the worst case this is $O(\|G\|^2)$ if the exemplar is the entire graph, $G$. In every realistic case, however, the exemplar will be markedly smaller than the approximately one hundred node graph. The algorithm requires $O(\|G\|+\|exemplar\|)$ space.

\section{Experiments}

Talk about broad classes of experiments -- static, moving, and AIBO

\subsection{Static Images}

Easy. Real and Computer-generated

\subsection{Moving Images}

GNG is able to follow what's going on. Very cool!

\subsection{Real-Time Object Tracking with AIBO}

Here's where the implicit memory really helps. As objects move and disappear, GNG is able to cull useless errors.

\subsection{Limitations}

No sense of edges or depth, so we perform poorly on complex, busy images. Example of busy image? Computer in Stromme's room?

\section{Future Work}

\begin{itemize}
  \item Reduce resolution
  \item Black and white only
  \item make error more precise as time goes on if not enough subgraphs
  \item more robust AIBO object-following mechanisms
\end{itemize}

\end{document}
